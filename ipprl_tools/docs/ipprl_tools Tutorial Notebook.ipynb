{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ipprl_tools Tutorial Notebook\n",
    "This notebook is a walk-through of the following topics:\n",
    "    1. Reading data using Pandas.\n",
    "    2. Using Synthetic Data Generation Methods.\n",
    "    3. Calculating Linkability Metrics on Generated Data.\n",
    "    4. Writing data and metric information to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ipprl_tools import synthetic,metrics\n",
    "from ipprl_tools.utils import tutorial,data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading Data Using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The module comes with a link to some pre-made synthetic data to demonstrate the corruption methods. To download it, we can use the `get_data()`  method from the `utils` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = tutorial.get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets us the path to the data that has been pre-downloaded.\n",
    "To read in the data, we use the `read_pickle()` method from `pandas`. We use this method because it can handle reading compressed ZIP files. If your data is in CSV format, you can also use `pandas.read_csv()` to read your data in.\n",
    "\n",
    "In either case, the `data` variable will contain a Pandas DataFrame object after calling.\n",
    "\n",
    "**Important Note:** In order for the corruption methods to work correctly, the DataFrame you use must be entirely of type `np.str`. The corruption methods expect to operate on strings, and many will break on non-string data. One easy way to make sure your DataFrame is of type `np.str` is to call the function `.astype(np.str)` when reading your data. This will cast all columns of the DataFrame to be of the correct type.\n",
    "\n",
    "We can also print out a sample of the data using `<DataFrame>.head(<num_rows>)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_pickle(path).astype(np.str)\n",
    "# Drop some of the unecessary columns in our dataset.\n",
    "raw_data = raw_data.drop([\"first_name\",\"first_name2\",\"last_name\",\"last_name2\",\"email\",\"address\",\"city2\",\"zip2\",\"state2\"],axis=1)\n",
    "# Rename the columns of our dataset.\n",
    "raw_data.columns = [\"first_name\",\"last_name\",\"email\",\"address\",\"ssn\",\"sex\",\"city\",\"zip\",\"state\",\"dob\",\"phone\",\"phone2\",\"phone3\",\"race\",\"pcp_npi\",\"suffix\",\"title\"]\n",
    "# Split the data into a dataset, and a swap set. We do this so that we can utilize the swap set in section 4.1.1\n",
    "dataset = raw_data.iloc[:400000]\n",
    "swap_set = raw_data.iloc[400000:]\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Synthetic Data Generation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is read in, we want to apply some corruption methods on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a copy of the first few rows of data here so that we can compare it to the non-corrupted version.\n",
    "data_to_corrupt = dataset.iloc[:5].copy()\n",
    "# The indicators dictionary will hold some information about the corruptions as they are performed.\n",
    "indicators = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we call the `drop_per_column()` method on our small amount of sample data. We pass the function:\n",
    "1. `data` - The DataFrame holding our data.\n",
    "2. `indicators` - A dictionary to hold some metadata about the corruptions.\n",
    "3. `columns` - We pass `columns = None` to signify that we want this operation to run on *all* columns in the DataFrame.\n",
    "4. `drop_pct` - This parameter tells the function what percentage of the rows should be dropped. In our case, we want to drop 50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic.drop_per_column(data=data_to_corrupt,indicators=indicators,columns=None,drop_pct=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare the original results to our corrupted version, we can see the the function has randomly deleted some elements of each row (The function rounded down from 50% to 2 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = data_to_corrupt.join(dataset.iloc[:5],lsuffix=\"_corrupt\")\n",
    "comparison[[\"first_name\",\"first_name_corrupt\",\"last_name\",\"last_name_corrupt\",\"address\",\"address_corrupt\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The indicators dictionary also contains information about which elements specifically were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_row(metadata, row,num_columns):\n",
    "    return [None if metadata.get((i,row)) is None else metadata.get((i,row)).keys() for i in range(num_columns)]\n",
    "\n",
    "def make_df_from_metadata(metadata,data):\n",
    "    num_columns = len(data.columns)\n",
    "    \n",
    "    metrics_df = pd.DataFrame.from_dict({idx : get_metrics_row(metadata,idx,num_columns) for idx in range(len(data))},orient=\"index\",columns=data.columns)\n",
    "    metrics_df[\"type\"] = \"metadata\"\n",
    "    \n",
    "    tmp_data = data.copy()\n",
    "    tmp_data[\"type\"] = \"data\"\n",
    "    \n",
    "    \n",
    "    visual_df = pd.concat([tmp_data,metrics_df]).set_index(\"type\",append=True).sort_index()\n",
    "    return visual_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the above helper functions above, we can view the corrupted data and the indicator metadata side-by-side. The indicator metadata records the corruptions, and in the case of more complex corruption methods, information about the corruption that was performed on each element of the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df = make_df_from_metadata(indicators,data_to_corrupt)\n",
    "meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Chaining Synthetic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a synthetic dataset suitable for linkage, we can call multiple synthetic data methods, one after another, on the same data. The end result of this chain is a dataset where multiple corruptions have been performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_corrupt_large = dataset.iloc[:50].copy()\n",
    "indicators_large = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below code, we chain together multiples calls to synthetic methods, passing the same data and indicator variables to each method. After calling the methods, we can print out the metadata DataFrame to see which corruptions were performed for each variable value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insrt_columns = [\"first_name\",\"last_name\",\"email\"]\n",
    "insrt_freqs = [0.2,0.2,0.5]\n",
    "insrt_nums = [2,2,4]\n",
    "synthetic.string_insert_alpha(data=data_to_corrupt_large,\n",
    "                              indicators=indicators_large,\n",
    "                              insrt_num=insrt_nums,\n",
    "                              insrt_freq=insrt_freqs,\n",
    "                              columns=insrt_columns)\n",
    "\n",
    "n_insrt_columns = [\"phone\",\"ssn\"]\n",
    "n_insrt_freqs = [0.1,0.2]\n",
    "n_insrt_nums = [2,2]\n",
    "synthetic.string_insert_numeric(data=data_to_corrupt_large,\n",
    "                                indicators=indicators_large,\n",
    "                                insrt_num=n_insrt_nums,\n",
    "                                insrt_freq=n_insrt_freqs,\n",
    "                                columns=n_insrt_columns)\n",
    "\n",
    "drop_cols = [\"first_name\",\"last_name\",\"email\",\"phone\",\"ssn\"]\n",
    "drop_freqs = [0.2,0.1,0.5,0.4,0.1]\n",
    "synthetic.drop_per_column(data=data_to_corrupt_large,\n",
    "                          indicators=indicators_large,\n",
    "                          columns=drop_cols,\n",
    "                          drop_pct=drop_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "large_meta_df = make_df_from_metadata(indicators_large,data_to_corrupt_large)\n",
    "large_meta_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this information by writing it to an Excel file using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_meta_df.to_excel(\"test_excel.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Calculating Linkability Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a dataset that has been sufficiently corrupted, we may want to calculate linkability measures on the data, to determine which columns we should use for linkage.\n",
    "\n",
    "We can calculate metrics on the data using the `metrics` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.run_metrics(data_to_corrupt_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the above DataFrame represents a column from the original dataset. The columns in the DataFrame are various Linkability Measures, which are calculated directly from the data. For more information about what these linkability measures mean, visit [this page.](https://github.com/cu-recordlinkage/iPPRL/blob/master/linkability/Metrics_Table.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Preparing Files for Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a dataset that we can use for linkage testing, we can use another function from the `utils.data` submodule.\n",
    "\n",
    "In this example, we are now operating on `data`, which is the complete tutorial dataset we read in at the start of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_ds,right_ds,gt_labels = data.split_dataset(dataset,overlap_pct=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above line of code, we used the `split_dataset` function from `ipprl_tools.utils.data` to split the dataset for us. This function accepts a set of data and splits it into two datasets, each of which has some unique rows, and some rows that overlap with the other dataset. The exact amount of overlap is configurable with the `overlap_pct` parameter.\n",
    "\n",
    "In this case, we chose to have 20% of the rows from `dataset` appear in both `left_ds` and `right_ds`. \n",
    "\n",
    "In addition to returning the two dataset variables, the function also returns a set of ground truth labels, `gt_labels`, which provide the `ID`s of the overlapping rows in `left_ds` and `right_ds`. If desired, you can evalaute the performance of your linkage using these known ground-truth labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Applying Corruption Methods\n",
    "Like in Section 3, we will now apply corruption methods to the synthetic data. \n",
    "\n",
    "This time, we must operate on two datasets, `left_ds` and `right_ds`.\n",
    "\n",
    "**Note:** These methods might take a long time to run, because they are operating on very large data. If you'd like them to finish quicker, you can pass a subset of the data (using the `.iloc` function of DataFrame) to the `split_dataset()` function above to make these operations complete quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_meta = {}\n",
    "synthetic.string_transpose(left_ds,left_meta,4,0.05)\n",
    "print(\"Transpose Complete.\")\n",
    "synthetic.string_delete(left_ds,left_meta,3,0.05)\n",
    "print(\"Delete Complete.\")\n",
    "synthetic.string_insert_alpha(left_ds,left_meta,3,0.05,columns=[\"first_name\",\"last_name\",\"email\",\"address\",\"city\",\"title\"])\n",
    "print(\"Insert Alpha Complete.\")\n",
    "synthetic.string_insert_numeric(left_ds,left_meta,3,0.05,columns=[\"phone\",\"phone2\",\"phone3\",\"zip\"])\n",
    "print(\"Insert Numeric Complete.\")\n",
    "synthetic.edit_values(left_ds,swap_set,left_meta,0.1)\n",
    "print(\"Edit Values Complete.\")\n",
    "\n",
    "columns = [\"first_name\",\n",
    "            \"last_name\",\n",
    "            \"email\",\n",
    "            \"address\",\n",
    "            \"ssn\",\n",
    "            \"sex\",\n",
    "            \"city\",\n",
    "            \"zip\",\n",
    "            \"state\",\n",
    "            \"dob\",\n",
    "            \"phone\",\n",
    "            \"phone2\",\n",
    "            \"phone3\",\n",
    "            \"race\",\n",
    "            \"pcp_npi\",\n",
    "            \"suffix\",\n",
    "            \"title\"]\n",
    "\n",
    "drop_pcts = [0.03,\n",
    "             0.03,\n",
    "             0.75,\n",
    "             0.06,\n",
    "             0.25,\n",
    "             0.07,\n",
    "             0.07,\n",
    "             0.07,\n",
    "             0.02,\n",
    "             0.02,\n",
    "             0.85,\n",
    "             0.85,\n",
    "             0.2,\n",
    "             0.2,\n",
    "             0.99,\n",
    "             0.2]\n",
    "\n",
    "synthetic.drop_per_column(left_ds,left_meta,drop_pct=drop_pcts,columns=columns)\n",
    "print(\"Per-Column Drop Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_meta = {}\n",
    "synthetic.string_transpose(right_ds,right_meta,4,0.05)\n",
    "print(\"Transpose Complete.\")\n",
    "synthetic.string_delete(right_ds,right_meta,3,0.05)\n",
    "print(\"Delete Complete.\")\n",
    "synthetic.string_insert_alpha(right_ds,right_meta,3,0.05,columns=[\"first_name\",\"last_name\",\"email\",\"address\",\"city\",\"title\"])\n",
    "print(\"Insert Alpha Complete.\")\n",
    "synthetic.string_insert_numeric(right_ds,right_meta,3,0.05,columns=[\"phone\",\"phone2\",\"phone3\",\"zip\"])\n",
    "print(\"Insert Numeric Complete.\")\n",
    "synthetic.edit_values(right_ds,swap_set,right_meta,0.1)\n",
    "print(\"Edit Values Complete.\")\n",
    "\n",
    "columns = [\"first_name\",\n",
    "            \"last_name\",\n",
    "            \"email\",\n",
    "            \"address\",\n",
    "            \"ssn\",\n",
    "            \"sex\",\n",
    "            \"city\",\n",
    "            \"zip\",\n",
    "            \"state\",\n",
    "            \"dob\",\n",
    "            \"phone\",\n",
    "            \"phone2\",\n",
    "            \"phone3\",\n",
    "            \"race\",\n",
    "            \"pcp_npi\",\n",
    "            \"suffix\",\n",
    "            \"title\"]\n",
    "\n",
    "r_drop_pcts = [0.05,\n",
    "             0.03,\n",
    "             0.75,\n",
    "             0.06,\n",
    "             0.25,\n",
    "             0.07,\n",
    "             0.07,\n",
    "             0.07,\n",
    "             0.02,\n",
    "             0.02,\n",
    "             0.80,\n",
    "             0.80,\n",
    "             0.2,\n",
    "             0.2,\n",
    "             0.99,\n",
    "             0.2]\n",
    "\n",
    "synthetic.drop_per_column(right_ds,right_meta,drop_pct=r_drop_pcts,columns=columns)\n",
    "print(\"Per-Column Drop Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that the corruption ran on both datasets, we can run the linkability metrics on both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.run_metrics(left_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.run_metrics(right_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the first few rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine these two datasets into a single dataset in order to use it as input for linkage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = pd.concat([left_ds,right_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `concat()` function will concatenate the two DataFrames into a single DataFrame along the axis. In our case, the `split_data()` utility function arranged it so that the indices of our index column `id`, are unique. If you did not use `split_data()` you'll want to make sure that you have references to the original IDs of your data so that you can evaluate the performance later.\n",
    "\n",
    "`full_ds` is now a DataFrame which contains `left_ds` and `right_ds` stacked on top of each other (concatenated along the row dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the ground truth IDs from `split_data()` are still valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_num = 1\n",
    "full_ds.loc[[gt_labels[pair_num][0],gt_labels[pair_num][1]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply call `.to_csv()` to save our new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds.to_csv(\"test_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate performance later, it is also a good idea to save the individual meta objects as well as the ground-truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# We can save the metadata files as .pkl files, which are a common binary format for Python.\n",
    "pickle.dump(left_meta,open(\"left_meta.pkl\",\"wb\"))\n",
    "pickle.dump(right_meta,open(\"right_meta.pkl\",\"wb\"))\n",
    "# We'll save the ground truth labels into a pikcle as well.\n",
    "pickle.dump(gt_labels,open(\"gt_labels.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open these files again later, we can use the `pickle.load()` function in the same way we just used `pickle.dump()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_read = pickle.load(open(\"gt_labels.pkl\",\"rb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
